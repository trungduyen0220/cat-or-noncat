{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from lr_utils import load_dataset\n",
    "from unit_test import *\n",
    "from dnn_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Datasets##\n",
    "\n",
    "Dataset (\"data.h5\") containing:\n",
    "    - a training set of m_train images labeled as cat (y=1) or non-cat (y=0)\n",
    "    - a test set of m_test images labeled as cat or non-cat\n",
    "    - each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes  = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the `index` value and re-run to see other images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 1. It's a cat picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 111\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 209\n",
      "Number of testing examples: m_test = 50\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (209, 64, 64, 3)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x shape: (50, 64, 64, 3)\n",
      "test_set_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num\\_px $*$ num\\_px $*$ 3, 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x_flatten shape: (12288, 209)\n",
      "train_set_y shape: (1, 209)\n",
      "test_set_x_flatten shape: (12288, 50)\n",
      "test_set_y shape: (1, 50)\n",
      "sanity check after reshaping: [17 31 56 22 33]\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Initialization\n",
    "##### Arguments:\n",
    "layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "\n",
    "##### Returns:\n",
    "parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                bl -- bias vector of shape (layer_dims[l], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Forward propagation module\n",
    "\n",
    "### 4.1 - Linear Forward \n",
    "Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "##### Arguments:\n",
    "A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "##### Returns:\n",
    "Z -- the input of the activation function, also called pre-activation parameter \n",
    "cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Linear-Activation Forward\n",
    "\n",
    "##### Arguments:\n",
    "A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "##### Returns:\n",
    "A -- the output of the activation function, also called the post-activation value \n",
    "cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "         stored for computing the backward pass efficiently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-Layer Model \n",
    "\n",
    "##### Arguments:\n",
    "X -- data, numpy array of shape (input size, number of examples)\n",
    "parameters -- output of initialize_parameters_deep()\n",
    "##### Returns:\n",
    "AL -- last post-activation value\n",
    "caches -- list of caches containing:\n",
    "            every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "   \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Cost function\n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))Â \\tag{7}$$\n",
    "\n",
    "Implement the cost function defined by equation (7).\n",
    "\n",
    "##### Arguments:\n",
    "AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "##### Returns:\n",
    "cost -- cross-entropy cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m)* np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1-Y, np.log(1-AL)))\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.2797765635793422\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Linear backward\n",
    "Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "#### Arguments:\n",
    "dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "#### Returns:\n",
    "dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "db -- Gradient of the cost with respect to b (current layer l), same shape as b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ, A_prev.T) / m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
      " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
      " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
      " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
      " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
      "dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
      " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
      " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
      "db = [[-0.14713786]\n",
      " [-0.11313155]\n",
      " [-0.13209101]]\n"
     ]
    }
   ],
   "source": [
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Linear-Activation backward\n",
    "Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "\n",
    "#### Arguments:\n",
    "dA -- post-activation gradient for current layer l \n",
    "cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "#### Returns:\n",
    "dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "db -- Gradient of the cost with respect to b (current layer l), same shape as b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, cache[1])\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, cache[1])\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - L-Model Backward \n",
    "Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "\n",
    "##### Arguments:\n",
    "AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "caches -- list of caches containing:\n",
    "            every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "            the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "\n",
    "##### Returns:\n",
    "grads -- A dictionary with the gradients\n",
    "         grads[\"dA\" + str(l)] = ... \n",
    "         grads[\"dW\" + str(l)] = ...\n",
    "         grads[\"db\" + str(l)] = ... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    dAL =  - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L - 1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] =  linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache =  caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - Update Parameters\n",
    " Update parameters using gradient descent\n",
    "##### Arguments:\n",
    "parameters -- python dictionary containing your parameters \n",
    "grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "##### Returns:\n",
    "parameters -- python dictionary containing your updated parameters \n",
    "              parameters[\"W\" + str(l)] = ... \n",
    "              parameters[\"b\" + str(l)] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "\n",
    "##### Arguments:\n",
    "X -- input data, of shape (n_x, number of examples)\n",
    "Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "num_iterations -- number of iterations of the optimization loop\n",
    "learning_rate -- learning rate of the gradient descent update rule\n",
    "print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "\n",
    "##### Returns:\n",
    "parameters -- a dictionary containing W1, W2, b1, and b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "        # Compute cost\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-f2297765406a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtwo_layer_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
